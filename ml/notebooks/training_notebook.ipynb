{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Network Traffic Anomaly Detection - Training\n",
                "This notebook covers the data loading, cleaning, robust feature engineering, and training of an optimized Isolation Forest model for AIOps-ready network telemetry analysis.\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "\n",
                "# Check environment (Colab vs Local)\n",
                "try:\n",
                "    from google.colab import files\n",
                "    IN_COLAB = True\n",
                "    print(\"Running in Google Colab environment.\")\n",
                "except ImportError:\n",
                "    IN_COLAB = False\n",
                "    print(\"Running in Local environment.\")\n",
                "\n",
                "# Visual settings\n",
                "sns.set(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "Automatically detects `data/network_traffic_data.csv` locally, or prompts for upload in Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_file = 'network_traffic_data.csv'\n",
                "local_path = '../../data/network_traffic_data.csv' # Relative path from ml/notebooks/\n",
                "\n",
                "if os.path.exists(input_file):\n",
                "    # File exists in current directory (e.g., uploaded to Colab root)\n",
                "    data_path = input_file\n",
                "elif os.path.exists(local_path):\n",
                "    # File exists in project structure\n",
                "    data_path = local_path\n",
                "elif IN_COLAB:\n",
                "    print(\"Data not found. Please upload network_traffic_data.csv\")\n",
                "    uploaded = files.upload()\n",
                "    data_path = input_file\n",
                "else:\n",
                "    raise FileNotFoundError(\"Could not find network_traffic_data.csv. Please ensure it is in the data/ directory.\")\n",
                "\n",
                "df = pd.read_csv(data_path)\n",
                "print(f\"Loaded {len(df)} records from {data_path}.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Cleaning & Feature Engineering\n",
                "We clean the data and transform features. We also visualize the data to understand distributions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle missing values\n",
                "df['dns_query'] = df['dns_query'].fillna('none')\n",
                "df = df.dropna(subset=['source_ip', 'dest_ip', 'dest_port', 'protocol'])\n",
                "\n",
                "# Encoding Categorical Features\n",
                "le_protocol = LabelEncoder()\n",
                "df['protocol_enc'] = le_protocol.fit_transform(df['protocol'])\n",
                "\n",
                "# Scaling Numeric Features\n",
                "scaler = StandardScaler()\n",
                "numeric_features = ['dest_port', 'bytes_sent', 'bytes_recv']\n",
                "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
                "\n",
                "# Feature Selection\n",
                "features = numeric_features + ['protocol_enc']\n",
                "X = df[features]\n",
                "print(f\"Feature matrix shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Optimized Model\n",
                "We use a **Robust Isolation Forest** configuration:\n",
                "*   `n_estimators=300`: More trees for better stability and convergence.\n",
                "*   `bootstrap=True`: Randomly samples independent subsets, reducing overfitting.\n",
                "*   `n_jobs=-1`: Utilizes all CPU cores for faster training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Optimized Isolation Forest model...\")\n",
                "model = IsolationForest(\n",
                "    n_estimators=300,\n",
                "    contamination='auto',\n",
                "    max_samples='auto',\n",
                "    bootstrap=True,\n",
                "    random_state=42,\n",
                "    n_jobs=-1\n",
                ")\n",
                "model.fit(X)\n",
                "\n",
                "# Evaluate\n",
                "predictions = model.predict(X)\n",
                "df['anomaly_score'] = model.decision_function(X)\n",
                "df['is_anomaly'] = predictions\n",
                "\n",
                "anomaly_count = (predictions == -1).sum()\n",
                "print(f\"Detected {anomaly_count} anomalies out of {len(df)} records ({anomaly_count/len(df):.2%}).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Evaluation & Visualization\n",
                "Analyzing the distribution of anomaly scores helps confirm if the model is effectively separating outliers (left tail) from normal traffic (right)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(df['anomaly_score'], bins=50, kde=True, color='purple')\n",
                "plt.axvline(x=model.offset_, color='red', linestyle='--', label=f'Threshold ({model.offset_:.3f})')\n",
                "plt.title('Distribution of Anomaly Scores (Lower = More Anomalous)')\n",
                "plt.xlabel('Anomaly Score')\n",
                "plt.ylabel('Frequency')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export Artifacts\n",
                "Save the robust model and preprocessing objects for the API. In a local environment, these are saved safely to `ml/models/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_artifacts = {\n",
                "    'model': model,\n",
                "    'scaler': scaler,\n",
                "    'le_protocol': le_protocol,\n",
                "    'features': features\n",
                "}\n",
                "\n",
                "output_dir = '../../ml/models/'\n",
                "if not os.path.exists(output_dir) and not IN_COLAB:\n",
                "    # Fallback if directory structure is different\n",
                "    output_dir = ''\n",
                "\n",
                "artifact_fname = 'anomaly_model.joblib'\n",
                "artifact_path = os.path.join(output_dir, artifact_fname)\n",
                "\n",
                "# Create directory if needed\n",
                "if output_dir:\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "joblib.dump(model_artifacts, artifact_path)\n",
                "print(f\"Model saved to {artifact_path}\")\n",
                "\n",
                "# Download back to local machine if in Colab\n",
                "if IN_COLAB:\n",
                "    files.download(artifact_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}