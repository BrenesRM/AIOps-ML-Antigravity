{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Network Traffic Anomaly Detection - Training\n",
                "This notebook covers the data loading, cleaning, robust feature engineering, and training of an optimized Isolation Forest model for AIOps-ready network telemetry analysis.\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Visual settings\n",
                "sns.set(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "Upload the `network_traffic_data.csv` file from your local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "uploaded = files.upload()\n",
                "df = pd.read_csv('network_traffic_data.csv')\n",
                "print(f\"Loaded {len(df)} records.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Cleaning & Feature Engineering\n",
                "We clean the data and transform features. We also visualize the data to understand distributions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle missing values\n",
                "df['dns_query'] = df['dns_query'].fillna('none')\n",
                "df = df.dropna(subset=['source_ip', 'dest_ip', 'dest_port', 'protocol'])\n",
                "\n",
                "# Encoding Categorical Features\n",
                "le_protocol = LabelEncoder()\n",
                "df['protocol_enc'] = le_protocol.fit_transform(df['protocol'])\n",
                "\n",
                "# Scaling Numeric Features\n",
                "scaler = StandardScaler()\n",
                "numeric_features = ['dest_port', 'bytes_sent', 'bytes_recv']\n",
                "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
                "\n",
                "# Feature Selection\n",
                "features = numeric_features + ['protocol_enc']\n",
                "X = df[features]\n",
                "print(f\"Feature matrix shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Optimized Model\n",
                "We use a **Robust Isolation Forest** configuration:\n",
                "*   `n_estimators=300`: More trees for better stability and convergence.\n",
                "*   `bootstrap=True`: Randomly samples independent subsets, reducing overfitting.\n",
                "*   `n_jobs=-1`: Utilizes all CPU cores for faster training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Optimized Isolation Forest model...\")\n",
                "model = IsolationForest(\n",
                "    n_estimators=300,\n",
                "    contamination='auto',\n",
                "    max_samples='auto',\n",
                "    bootstrap=True,\n",
                "    random_state=42,\n",
                "    n_jobs=-1\n",
                ")\n",
                "model.fit(X)\n",
                "\n",
                "# Evaluate\n",
                "predictions = model.predict(X)\n",
                "df['anomaly_score'] = model.decision_function(X)\n",
                "df['is_anomaly'] = predictions\n",
                "\n",
                "anomaly_count = (predictions == -1).sum()\n",
                "print(f\"Detected {anomaly_count} anomalies out of {len(df)} records ({anomaly_count/len(df):.2%}).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Evaluation & Visualization\n",
                "Analyzing the distribution of anomaly scores helps confirm if the model is effectively separating outliers (left tail) from normal traffic (right)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(df['anomaly_score'], bins=50, kde=True, color='purple')\n",
                "plt.axvline(x=model.offset_, color='red', linestyle='--', label=f'Threshold ({model.offset_:.3f})')\n",
                "plt.title('Distribution of Anomaly Scores (Lower = More Anomalous)')\n",
                "plt.xlabel('Anomaly Score')\n",
                "plt.ylabel('Frequency')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export Artifacts\n",
                "Save the robust model and preprocessing objects for the API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_artifacts = {\n",
                "    'model': model,\n",
                "    'scaler': scaler,\n",
                "    'le_protocol': le_protocol,\n",
                "    'features': features\n",
                "}\n",
                "\n",
                "artifact_path = 'anomaly_model.joblib'\n",
                "joblib.dump(model_artifacts, artifact_path)\n",
                "print(f\"Model saved to {artifact_path}\")\n",
                "\n",
                "# Download back to local machine\n",
                "files.download(artifact_path)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}